{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a408bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from testCasesGC import *\n",
    "from gc_utils import sigmoid,relu,dictionary_to_vector,vector_to_dictionary,gradients_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a4a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(x,theta):\n",
    "    j=theta*x\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7fcb0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J=8\n"
     ]
    }
   ],
   "source": [
    "x,theta=2,4\n",
    "j=forward_propogation(x,theta)\n",
    "print('J='+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f349c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x,theta):\n",
    "    dtheta=x\n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef24f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta=2\n"
     ]
    }
   ],
   "source": [
    "x,theta=2,4\n",
    "dtheta=backward_propagation(x,theta)\n",
    "print('dtheta='+str(dtheta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404f27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x,theta,epsilon=1e-7,print_msg=False):\n",
    "    theta_plus=theta+epsilon\n",
    "    theta_minus=theta-epsilon\n",
    "    j_plus=forward_propogation(x,theta_plus)\n",
    "    j_minus=forward_propogation(x,theta_minus)\n",
    "    gradapprox=(j_plus-j_minus)/(2*epsilon)\n",
    "    \n",
    "    grad=backward_propagation(x,theta)\n",
    "    \n",
    "    numerator=np.linalg.norm(grad-gradapprox)\n",
    "    denominator=np.linalg.norm(grad)+np.linalg.norm(gradapprox)\n",
    "    difference=numerator/denominator\n",
    "    \n",
    "    if print_msg:\n",
    "        if difference>2e-7:\n",
    "            print('There is  a mistake in the back prop difference'+str(difference))\n",
    "        else:\n",
    "            print('back prop works fine differnce'+str(difference))\n",
    "            \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43713a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "back prop works fine differnce2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "#testing if this works\n",
    "x,theta=2,4\n",
    "difference=gradient_check(2,4,print_msg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97213535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X,Y,parameters):\n",
    "    m=X.shape[1]\n",
    "    W1=parameters['W1']\n",
    "    b1=parameters['b1']\n",
    "    W2=parameters['W2']\n",
    "    b2=parameters['b2']\n",
    "    W3=parameters['W3']\n",
    "    b3=parameters['b3']\n",
    "    \n",
    "    Z1=np.dot(W1,X)+b1\n",
    "    A1=relu(Z1)\n",
    "    Z2=np.dot(W2,A1)+b2\n",
    "    A2=relu(Z2)\n",
    "    Z3=np.dot(W3,A2)+b3\n",
    "    A3=sigmoid(Z3)\n",
    "    \n",
    "    log_probs=np.multiply(-np.log(A3),Y)+np.multiply(-np.log(1-A3),1-Y)\n",
    "    cost=1./m*np.sum(log_probs)\n",
    "    \n",
    "    cache=(Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3)\n",
    "    \n",
    "    return cost,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02544131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X,Y,cache):\n",
    "    m=X.shape[1]\n",
    "    (Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3)=cache\n",
    "    \n",
    "    dZ3=A3-Y\n",
    "    dW3=1./m*np.dot(dZ3,A2.T)\n",
    "    db3=1./m*np.sum(dZ3,axis=1,keepdims=True)\n",
    "    \n",
    "    dA2=np.dot(W3.T,dZ3)\n",
    "    dZ2=np.multiply(dA2,np.int64(A2>0))\n",
    "    dW2=1./m*np.dot(dZ2,A1.T)*2 #mistake here *2 not required\n",
    "    \n",
    "    #dW2=1./m*np.dot(dZ2,A1.T) #without mistake\n",
    "    \n",
    "    db2=1./m*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    \n",
    "    dA1=np.dot(W2.T,dZ2)\n",
    "    dZ1=np.multiply(dA1,np.int64(A1>0))\n",
    "    dW1=1./m*np.dot(dZ1,X.T)\n",
    "    db1=1./m*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    \n",
    "    gradients={'dZ3':dZ3,'dW3':dW3,'db3':db3,'dA2':dA2,\n",
    "              'dZ2':dZ2,'dW2':dW2,'db2':db2,'dA1':dA1,\n",
    "              'dZ1':dZ1,'dW1':dW1,'db1':db1}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18426852",
   "metadata": {},
   "source": [
    "> Gradient check -Check any errors ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39c4b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters,gradient_check_nradients,X,Y,epsilon=1e-7,print_msg=False):\n",
    "    \n",
    "    parameters_values,_=dictionary_to_vector(parameters)\n",
    "    \n",
    "    grads=gradients_to_vector(gradients)\n",
    "    num_parameters=parameters_values.shape[0]\n",
    "    j_plus=np.zeros((num_parameters,1))\n",
    "    j_minus=np.zeros((num_parameters,1))\n",
    "    gradapprox=np.zeros((num_parameters,1))\n",
    "    \n",
    "    \n",
    "    for i in range(num_parameters):\n",
    "        theta_plus=np.copy(parameters_values)\n",
    "        theta_plus[i][0]=theta_plus[i][0]+epsilon\n",
    "        j_plus[i],_=forward_propagation_n(X,Y,vector_to_dictionary(theta_plus))\n",
    "        \n",
    "        theta_minus=np.copy(parameters_values)\n",
    "        theta_minus[i][0]=theta_minus[i][0]-epsilon\n",
    "        j_minus[i],_=forward_propagation_n(X,Y,vector_to_dictionary(theta_minus))\n",
    "        \n",
    "        gradapprox[i]=(j_plus[i]-j_minus[i])/(2*epsilon)\n",
    "        \n",
    "        \n",
    "    numerator=np.linalg.norm(grads-gradapprox)\n",
    "    denominator=np.linalg.norm(grads)+np.linalg.norm(gradapprox)\n",
    "    difference=numerator/denominator\n",
    "    \n",
    "    if print_msg:\n",
    "        if difference>2e-7:\n",
    "            print('There is  a mistake in the back prop difference'+str(difference))\n",
    "        else:\n",
    "            print('back prop works fine differnce'+str(difference))\n",
    "            \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0c65c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is  a mistake in the back prop difference0.17344196871198458\n"
     ]
    }
   ],
   "source": [
    "X,Y,parameters=gradient_check_n_test_case()\n",
    "\n",
    "cost,cache=forward_propagation_n(X,Y,parameters)\n",
    "gradients=backward_propagation_n(X,Y,cache)\n",
    "difference=gradient_check_n(parameters,gradients,X,Y,1e-7,True)\n",
    "expected_values=[0.2850931567761623,1.189091302422996e-07]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8ec8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## shows the mistake in these code ...by gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f2140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
